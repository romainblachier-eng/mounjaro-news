#!/usr/bin/env python3
"""
Mounjaro News â€” GÃ©nÃ©rateur d'articles quotidiens bilingues
==========================================================
1. Scrape les derniÃ¨res actualitÃ©s Mounjaro via Google News RSS + PubMed RSS
2. Filtre les articles dÃ©jÃ  traitÃ©s (fichier .processed_urls)
3. Pour chaque nouvelle source :
   - RÃ©cupÃ¨re le contenu de l'article
   - GÃ©nÃ¨re via Claude AI :
       * Un rÃ©sumÃ© reformulÃ© en FRANÃ‡AIS (200-250 mots)
       * Une traduction/reformulation en ANGLAIS (200-250 mots)
4. CrÃ©e un fichier Hugo Markdown avec les deux versions en frontmatter

Variables d'environnement :
  ANTHROPIC_API_KEY   ClÃ© API Anthropic (obligatoire)
"""

import os
import sys
import json
import time
import hashlib
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

import requests
import feedparser


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RSS_FEEDS = [
    # Google News â€” Mounjaro EN
    "https://news.google.com/rss/search?q=mounjaro+tirzepatide&hl=en-US&gl=US&ceid=US:en",
    # Google News â€” Mounjaro FR
    "https://news.google.com/rss/search?q=mounjaro+tirzepatide&hl=fr&gl=FR&ceid=FR:fr",
    # PubMed â€” tirzepatide
    "https://pubmed.ncbi.nlm.nih.gov/rss/search/?term=tirzepatide&limit=5&format=abstract",
]

MAX_ARTICLES_PER_RUN = 3          # Nb max d'articles gÃ©nÃ©rÃ©s par exÃ©cution
PROCESSED_FILE       = ".processed_urls"
CONTENT_DIR          = Path("content/posts")
MIN_TITLE_LENGTH     = 20         # Ignore les titres trop courts


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Gestion des URLs dÃ©jÃ  traitÃ©es
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_processed() -> set:
    if Path(PROCESSED_FILE).exists():
        return set(Path(PROCESSED_FILE).read_text().splitlines())
    return set()

def save_processed(urls: set) -> None:
    Path(PROCESSED_FILE).write_text("\n".join(sorted(urls)))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Scraping des flux RSS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_rss_articles() -> list[dict]:
    """RÃ©cupÃ¨re les articles depuis tous les flux RSS."""
    articles = []
    seen_titles = set()

    for feed_url in RSS_FEEDS:
        try:
            print(f"  ğŸ“¡ {feed_url[:70]}â€¦")
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:10]:
                title = entry.get("title", "").strip()
                url   = entry.get("link", "").strip()
                # Nettoie les titres Google News (format "Titre - Source")
                title = re.sub(r'\s+[-â€“]\s+\S+\s*$', '', title).strip()

                if not title or not url or len(title) < MIN_TITLE_LENGTH:
                    continue
                if title.lower() in seen_titles:
                    continue

                seen_titles.add(title.lower())
                articles.append({
                    "title":       title,
                    "url":         url,
                    "source_name": urlparse(url).netloc.replace("www.", ""),
                    "summary":     entry.get("summary", ""),
                    "published":   entry.get("published", ""),
                })
        except Exception as e:
            print(f"  âš ï¸  Erreur RSS ({feed_url[:50]}â€¦) : {e}")

    return articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. RÃ©cupÃ©ration du contenu de l'article
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_article_content(url: str) -> str:
    """Tente de rÃ©cupÃ©rer le texte de l'article source (best-effort)."""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (compatible; MounjaroNewsBot/1.0)"}
        resp = requests.get(url, headers=headers, timeout=15)
        if resp.status_code != 200:
            return ""
        # Extraction brutale du texte (pas de BeautifulSoup pour rÃ©duire les dÃ©pendances)
        text = re.sub(r'<script[^>]*>.*?</script>', '', resp.text, flags=re.DOTALL)
        text = re.sub(r'<style[^>]*>.*?</style>',  '', text,      flags=re.DOTALL)
        text = re.sub(r'<[^>]+>',                  ' ', text)
        text = re.sub(r'\s+',                       ' ', text).strip()
        return text[:4000]  # Claude reÃ§oit les 4000 premiers caractÃ¨res
    except Exception:
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. GÃ©nÃ©ration bilingue avec Claude
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_bilingual_content(article: dict, content: str, api_key: str) -> tuple[str, str]:
    """
    Retourne (resume_fr, resume_en) â€” deux reformulations indÃ©pendantes.
    """
    import anthropic

    client  = anthropic.Anthropic(api_key=api_key)
    context = content if content else article.get("summary", article["title"])

    prompt = f"""Tu es un journaliste de santÃ© spÃ©cialisÃ© dans les mÃ©dicaments GLP-1.
On te donne un article sur le Mounjaro (tirzÃ©patide).
Produis UNIQUEMENT un JSON avec deux champs :
- "fr" : reformulation en franÃ§ais (200-250 mots), claire, factuelle, accessible au grand public
- "en" : reformulation en anglais (200-250 mots), clear, factual, accessible

RÃ¨gles :
- Ne jamais reproduire le texte original mot pour mot
- Pas de conseil mÃ©dical, pas de recommandation de traitement
- Si l'info est scientifique, la vulgariser
- Commencer "fr" par une phrase d'accroche forte
- Commencer "en" par a strong hook sentence

Titre de l'article : {article['title']}
Source : {article['source_name']}
Contenu disponible : {context[:3000]}

RÃ©ponds UNIQUEMENT avec le JSON, sans markdown ni commentaire."""

    try:
        message = client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=700,
            messages=[{"role": "user", "content": prompt}],
        )
        raw = message.content[0].text.strip()
        # Nettoyage au cas oÃ¹ Claude ajoute des backticks
        raw = re.sub(r'^```[a-z]*\n?', '', raw)
        raw = re.sub(r'\n?```$',       '', raw)
        data = json.loads(raw)
        return data.get("fr", ""), data.get("en", "")
    except json.JSONDecodeError:
        # Fallback si JSON malformÃ©
        return raw[:500], ""
    except Exception as e:
        print(f"    âš ï¸  Erreur Claude : {e}")
        return "", ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. CrÃ©ation du fichier Hugo Markdown
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def slugify(text: str) -> str:
    text = text.lower()
    text = re.sub(r'[Ã Ã¡Ã¢Ã£Ã¤Ã¥]', 'a', text)
    text = re.sub(r'[Ã¨Ã©ÃªÃ«]',   'e', text)
    text = re.sub(r'[Ã¬Ã­Ã®Ã¯]',   'i', text)
    text = re.sub(r'[Ã²Ã³Ã´ÃµÃ¶]',  'o', text)
    text = re.sub(r'[Ã¹ÃºÃ»Ã¼]',   'u', text)
    text = re.sub(r'[Ã§]',      'c', text)
    text = re.sub(r'[^a-z0-9]+', '-', text)
    return text.strip('-')[:60]


def escape_yaml(s: str) -> str:
    """Ã‰chappe une chaÃ®ne pour un champ YAML inline."""
    s = s.replace('\\', '\\\\').replace('"', '\\"')
    return s


def create_hugo_article(article: dict, fr_content: str, en_content: str) -> bool:
    """GÃ©nÃ¨re le fichier Markdown Hugo. Retourne True si crÃ©Ã©."""
    now      = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    slug     = f"{date_str}-{slugify(article['title'])}"
    filepath = CONTENT_DIR / f"{slug}.md"

    if filepath.exists():
        return False

    # RÃ©sumÃ© court pour la carte d'accueil (premiÃ¨re phrase du fr)
    summary_fr = (fr_content.split('.')[0] + '.') if fr_content else article['title']

    # Conversion du contenu en YAML multiline (| block scalar)
    def to_yaml_block(text: str) -> str:
        if not text:
            return '""'
        lines = text.replace('\r', '').split('\n')
        indented = '\n'.join('  ' + l for l in lines)
        return '|\n' + indented

    frontmatter = f"""---
title: "{escape_yaml(article['title'])}"
date: {now.strftime("%Y-%m-%dT%H:%M:%S")}+01:00
draft: false
description: "{escape_yaml(summary_fr[:160])}"
source_name: "{escape_yaml(article['source_name'])}"
source_url: "{article['url']}"
summary_fr: "{escape_yaml(summary_fr)}"
content_fr: {to_yaml_block(fr_content)}
content_en: {to_yaml_block(en_content)}
---
"""

    # Corps Markdown minimal (le vrai contenu est dans le frontmatter)
    body = f"""<!-- Article gÃ©nÃ©rÃ© automatiquement par Mounjaro News -->
<!-- Source : [{article['source_name']}]({article['url']}) -->
"""

    CONTENT_DIR.mkdir(parents=True, exist_ok=True)
    filepath.write_text(frontmatter + body, encoding="utf-8")
    print(f"  âœ… Article crÃ©Ã© : {filepath.name}")
    return True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Point d'entrÃ©e
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("  Mounjaro News â€” GÃ©nÃ©ration quotidienne   ")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    api_key = os.environ.get("ANTHROPIC_API_KEY", "").strip()
    if not api_key:
        print("âŒ Variable ANTHROPIC_API_KEY manquante.")
        sys.exit(1)

    processed = load_processed()

    # 1. Collecte des articles
    print("\nğŸ“¡ Collecte des flux RSSâ€¦")
    candidates = fetch_rss_articles()
    print(f"   {len(candidates)} articles trouvÃ©s")

    # 2. Filtrage des dÃ©jÃ  traitÃ©s
    new_articles = [a for a in candidates if a["url"] not in processed]
    print(f"   {len(new_articles)} nouveaux articles Ã  traiter")

    if not new_articles:
        print("\nâ„¹ï¸  Aucun nouvel article. Fin du script.")
        return

    # 3. Traitement
    created = 0
    for article in new_articles[:MAX_ARTICLES_PER_RUN]:
        print(f"\nğŸ” Traitement : {article['title'][:70]}")

        # RÃ©cupÃ©ration du contenu
        print("   â¬‡ï¸  RÃ©cupÃ©ration du contenuâ€¦")
        content = fetch_article_content(article["url"])
        if content:
            print(f"   âœ“ {len(content)} caractÃ¨res rÃ©cupÃ©rÃ©s")

        # GÃ©nÃ©ration bilingue
        print("   ğŸ¤– GÃ©nÃ©ration bilingue (Claude)â€¦")
        fr, en = generate_bilingual_content(article, content, api_key)

        if not fr:
            print("   âš ï¸  Contenu vide â€” article ignorÃ©.")
            processed.add(article["url"])
            continue

        # CrÃ©ation du fichier Hugo
        if create_hugo_article(article, fr, en):
            created += 1
            processed.add(article["url"])
        else:
            print(f"   â„¹ï¸  DÃ©jÃ  existant â€” ignorÃ©.")

        time.sleep(2)  # Pause entre les appels API

    save_processed(processed)
    print(f"\nâœ“ {created} article(s) crÃ©Ã©(s). TerminÃ©.")


if __name__ == "__main__":
    main()
